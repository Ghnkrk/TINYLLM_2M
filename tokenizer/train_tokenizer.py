# tokenizer/train_tokenizer.py
import json
import os
from datasets import load_dataset
import sentencepiece as spm


# -------------------------------------------------
# Load config
# -------------------------------------------------

def load_config(path="config.json"):
    with open(path, "r") as f:
        return json.load(f)


# -------------------------------------------------
# Load raw dataset (same logic as data.py)
# -------------------------------------------------

def load_texts(cfg):
    name = cfg["dataset"]["name"]

    if name == "tinystories":
        ds = load_dataset("roneneldan/TinyStories", split="train")
        text_col = "text"

    elif name == "simplewiki":
        ds = load_dataset("wikipedia", "20220301.simple", split="train")
        text_col = "text"

    else:
        raise ValueError(f"Unknown dataset: {name}")

    max_samples = cfg["dataset"].get("max_samples")
    if max_samples:
        ds = ds.shuffle(seed=42).select(range(max_samples))

    texts = []
    for ex in ds:
        t = ex[text_col].replace("\n", " ").strip()
        if len(t) > 20:
            texts.append(t)

    return texts


# -------------------------------------------------
# Train SentencePiece tokenizer
# -------------------------------------------------

def train_tokenizer():
    cfg = load_config()

    os.makedirs("tokenizer", exist_ok=True)

    vocab_size = cfg["tokenizer"]["vocab_size"]
    model_prefix = cfg["tokenizer"]["model_path"].replace(".model", "")

    print(f"\nðŸ”¤ Training SentencePiece tokenizer")
    print(f"Vocab size: {vocab_size}")
    print(f"Saving to: {model_prefix}.model\n")

    texts = load_texts(cfg)

    # write temp corpus
    corpus_path = "tokenizer/corpus.txt"
    with open(corpus_path, "w", encoding="utf-8") as f:
        for line in texts:
            f.write(line + "\n")

    spm.SentencePieceTrainer.train(
        input=corpus_path,
        model_prefix=model_prefix,
        vocab_size=vocab_size,
        model_type="bpe",
        character_coverage=1.0,
        split_digits=True,
        normalization_rule_name="nmt_nfkc",
        bos_id=-1,
        eos_id=-1,
        pad_id=0,
        unk_id=1
    )

    os.remove(corpus_path)
    print("âœ… Tokenizer training complete")


if __name__ == "__main__":
    train_tokenizer()
